{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c0cd078",
   "metadata": {},
   "source": [
    "# Part A - (1) Feature Extraction\n",
    "The code has been modified to split the data into 3 sets: Training (70%), Calibration (15%) and Testing (15%) using sklearn's test_train_split function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac64b92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import timm\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Load pretrained Vision Transformer\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "vit = timm.create_model(\"vit_base_patch16_224\", pretrained=True).to(device)\n",
    "vit.eval() # disable dropout, etc.\n",
    "\n",
    "# 2. Define preprocessing consistent with ImageNet training\n",
    "preprocess = transforms.Compose([\n",
    "transforms.Resize(256),\n",
    "transforms.CenterCrop(224),\n",
    "transforms.ToTensor(),\n",
    "transforms.Normalize(\n",
    "mean=[0.485, 0.456, 0.406],\n",
    "std=[0.229, 0.224, 0.225]\n",
    "),\n",
    "])\n",
    "\n",
    "# 3. Directory containing CelebA images (modify path)\n",
    "image_dir = \"/home/arty/projects/cs342_dataset/img_align_celeba\"\n",
    "image_list = sorted(os.listdir(image_dir))[:20000] # sample subset\n",
    "\n",
    "# Get labeled attributes from dataset\n",
    "attr_file = \"/home/arty/projects/cs342_dataset/list_attr_celeba.csv\"\n",
    "attr_df = pd.read_csv(attr_file)\n",
    "\n",
    "# Split images into train (70%). calib (15%), test (15%)\n",
    "train_set, remaining_set = train_test_split(\n",
    "    image_list, \n",
    "    test_size=0.30, \n",
    "    random_state=42, # Use a fixed random_state for reproducibility\n",
    "    shuffle=True\n",
    ")\n",
    "calib_set, test_set = train_test_split(\n",
    "    remaining_set, \n",
    "    test_size=0.50, # 50% of the remaining 30% = 15% of the total\n",
    "    random_state=42, # Using the same random_state is good practice\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "sets = [train_set, calib_set, test_set]\n",
    "\n",
    "# 4. Extract embeddings\n",
    "for split_name, image_list in zip([\"train\", \"calib\", \"test\"], sets):\n",
    "    print(f\"Processing {split_name} set with {len(image_list)} images...\")\n",
    "\n",
    "    all_features = []\n",
    "    with torch.no_grad():\n",
    "        for fname in tqdm(image_list):\n",
    "            img = Image.open(os.path.join(image_dir, fname)).convert(\"RGB\")\n",
    "            x = preprocess(img).unsqueeze(0).to(device)\n",
    "            features = vit.forward_features(x)[:, 0, :] # shape: (1, 768)\n",
    "            all_features.append(features.cpu().numpy())\n",
    "    all_features = np.concatenate(all_features, axis=0)\n",
    "\n",
    "    # Add 'Smiling' class label as the last column\n",
    "    smiling_labels = []\n",
    "    for fname in image_list:\n",
    "        index = int(fname.split('.')[0]) \n",
    "        smiling_label = 1 if attr_df.at[index, 'Smiling'] == 1 else 0\n",
    "        smiling_labels.append(smiling_label)\n",
    "    smiling_labels = np.array(smiling_labels).reshape(-1, 1)\n",
    "    all_features = np.hstack((all_features, smiling_labels))\n",
    "\n",
    "    print(\"Feature matrix shape:\", all_features.shape) # e.g. (20000, 768 + 1)\n",
    "\n",
    "    # 5. Save for later use\n",
    "    np.save(f\"celeba_vit_embeddings_{split_name}.npy\", all_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0064d",
   "metadata": {},
   "source": [
    "# Part A - (2) Train Classifiers\n",
    "Implement a Naive Bayes classifier to predict the Smiling attribute\n",
    "(1 = smiling, 0 = not smiling) and report test accuracy. The Naive Bayes classifier should\n",
    "be implemented by you and not be a call to a library function. The features here\n",
    "are numerical (not categorical). So conditioned on the label, model each feature as an\n",
    "independent Gaussian. Your algorithm should estimate the mean and variance of each of\n",
    "these Gaussians, using the training data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b4debc27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(-1475.2638435704364), np.float64(-1486.0390792175162))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from math import exp, sqrt, log, pi\n",
    "\n",
    "train_data = pd.DataFrame(np.load(\"./celeba_vit_embeddings_train.npy\"))\n",
    "train_data.head()\n",
    "\n",
    "num_features = train_data.shape[1] - 1\n",
    "\n",
    "# First separate the data by class\n",
    "pos_df = train_data[train_data[num_features]==1]\n",
    "neg_df = train_data[train_data[num_features]==0]\n",
    "\n",
    "# Now we calculate the mean and std deviation for each feature in both classes\n",
    "pos_std_devs = pos_df.std()\n",
    "pos_means = pos_df.mean()\n",
    "neg_std_devs = neg_df.std()\n",
    "neg_means = neg_df.mean()\n",
    "\n",
    "# Calculate PDF for a given input feature vector\n",
    "def calc_probs_vectorized(vector):\n",
    "    total_rows = train_data.shape[0]\n",
    "    \n",
    "    # Prior probabilities\n",
    "    log_pos_prior = np.log(pos_df.shape[0] / total_rows)\n",
    "    log_neg_prior = np.log(neg_df.shape[0] / total_rows)\n",
    "    \n",
    "    # Vectorized Gaussian log probability calculation\n",
    "    def gaussian_log_prob(vector, means, stddevs):\n",
    "        # Add small epsilon to avoid zero stddev\n",
    "        stddevs = np.where(stddevs == 0, 1e-9, stddevs)\n",
    "        return -0.5 * np.log(2 * pi) - np.log(stddevs) - ((vector - means) ** 2) / (2 * stddevs ** 2)\n",
    "    \n",
    "    # Calculate log probabilities for all features\n",
    "    pos_log_probs = gaussian_log_prob(vector[:num_features], pos_means[:num_features], pos_std_devs[:num_features])\n",
    "    neg_log_probs = gaussian_log_prob(vector[:num_features], neg_means[:num_features], neg_std_devs[:num_features])\n",
    "    \n",
    "    # Sum all feature log probabilities and add priors\n",
    "    log_pos_prob = log_pos_prior + np.sum(pos_log_probs)\n",
    "    log_neg_prob = log_neg_prior + np.sum(neg_log_probs)\n",
    "    \n",
    "    return log_pos_prob, log_neg_prob\n",
    "    \n",
    "# Run predictions on the training set\n",
    "print(calc_probs(list(pos_df.iloc[1])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
